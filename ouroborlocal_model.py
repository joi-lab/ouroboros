# ouroboros/local_model.py
from __future__ import annotations
import logging, os, signal, subprocess, sys, time
from typing import Optional
import requests
from huggingface_hub import hf_hub_download

log = logging.getLogger(__name__)

class LocalModelManager:
    def __init__(self):
        self._proc: Optional[subprocess.Popen] = None
        self._port = 8000

    def download_model(self, repo_id: str, filename: str) -> str:
        log.info(f"üì• –°–∫–∞—á–∏–≤–∞—é {repo_id}/{filename} (\~21 –ì–ë, 10‚Äì30 –º–∏–Ω)...")
        return hf_hub_download(repo_id=repo_id, filename=filename, resume_download=True)

    def start_server(self, model_path: str):
        if self._proc and self._proc.poll() is None:
            return

        n_gpu_layers = int(os.getenv("N_GPU_LAYERS", "40"))   # 35‚Äì45 –¥–ª—è T4
        n_ctx = int(os.getenv("LOCAL_CTX", "8192"))

        cmd = [
            sys.executable, "-m", "llama_cpp.server",
            "--model", model_path,
            "--port", str(self._port),
            "--n_gpu_layers", str(n_gpu_layers),
            "--n_ctx", str(n_ctx),
            "--chat_format", "qwen",          # —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –¥–ª—è Qwen3.5
            "--host", "0.0.0.0",
            "--n_batch", "512",
            "--n_threads", "8",
            "--verbose", "false"
        ]

        self._proc = subprocess.Popen(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL,
                                      start_new_session=True)
        log.info(f"üöÄ –ó–∞–ø—É—Å–∫–∞—é llama-server (n_gpu_layers={n_gpu_layers}, ctx={n_ctx})...")

        for _ in range(90):  # –¥–æ 7.5 –º–∏–Ω—É—Ç
            try:
                r = requests.get(f"http://127.0.0.1:{self._port}/v1/models", timeout=5)
                if r.status_code == 200:
                    log.info("‚úÖ Local Qwen3.5-35B-A3B —Å–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤!")
                    return
            except:
                time.sleep(5)
        raise RuntimeError("‚ùå –°–µ—Ä–≤–µ—Ä –Ω–µ –∑–∞–ø—É—Å—Ç–∏–ª—Å—è")

    def stop(self):
        if self._proc:
            os.killpg(os.getpgid(self._proc.pid), signal.SIGTERM)
            self._proc = None

get_local_manager = LocalModelManager
